# .github/workflows/news_scraper.yml

name: Real-time News Scraper and Dashboard Update

on:
  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

  # Schedule to run every 15 minutes for closer to real-time updates
  schedule:
    - cron: '*/15 * * * *' # At every 15th minute.

jobs:
  run-and-commit-news-updates:
    runs-on: ubuntu-latest

    # Ensure only one workflow instance runs at a time for this job
    concurrency:
      group: news_scraper_dashboard_update
      cancel-in-progress: true

    # Permissions needed for committing changes
    permissions:
      contents: write # To commit changes back to the repository

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9' # Or '3.10', '3.11' based on preference

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      - name: Run news.py script
        run: python news.py

      - name: Configure Git
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"

      - name: Check for changes and commit
        id: commit_changes
        run: |
          # Add all relevant files and directories to staging
          git add news_log.txt news_dashboard.html scraped_articles_raw_text/ article_pages/
          # Check if there are any pending changes to commit (returns non-zero if changes exist)
          if ! git diff --cached --exit-code; then
            git commit -m "Automated: Update news dashboard and scraped articles"
            git push
            echo "Committed and pushed changes."
            echo "committed=true" >> $GITHUB_OUTPUT
          else
            echo "No changes to commit. Skipping commit step."
            echo "committed=false" >> $GITHUB_OUTPUT
          fi
